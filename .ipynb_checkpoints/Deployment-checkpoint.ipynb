{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment.ipynb\n",
    "<hr>\n",
    "\n",
    "### File containting all the deployment codes\n",
    "\n",
    "This notebook contains all the contents of respective files that were required to deploy the model, for predicting time_to_eruption of volcano, on heroku platform\n",
    "\n",
    "The files that are required are listed below along with its content in respective code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) app.py\n",
    "Contains the main streamlit python app code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from model import *\n",
    "from predict import *\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    st.set_page_config(layout=\"wide\")\n",
    "\n",
    "    st.title(\"Volcanic Eruption Prediction from Seismic Signals\")\n",
    "    image_url = \"https://images.unsplash.com/photo-1519901416153-b3ea11f3fcc6?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1350&q=80\"\n",
    "    st.image(image_url, width=800)\n",
    "\n",
    "    st.sidebar.header(\"About\")\n",
    "\n",
    "    with open('about.txt', 'r') as f:\n",
    "        about_txt = f.read()\n",
    "\n",
    "    st.sidebar.write(about_txt)\n",
    "\n",
    "    st.text(\"\")\n",
    "    st.text(\"\")\n",
    "    st.text(\"\")\n",
    "\n",
    "    exp = st.beta_expander(\"Instructions : \", expanded=True)\n",
    "    with exp:\n",
    "        \"1. Please upload a csv file in the file uploader.\"\n",
    "        \"2. File should contain the 10-D sensor data.\"\n",
    "        \"3. Visualize the time series by toggling appropriate options.\"\n",
    "\n",
    "    st.text(\"\")\n",
    "    st.text(\"\")\n",
    "    st.text(\"\")\n",
    "    st.text(\"\")\n",
    "\n",
    "    st.subheader(\"File Uploader :\")\n",
    "    csv = st.file_uploader(\"Upload a segment file containing the seismic data from 10 sensors\", type=['csv'])\n",
    "\n",
    "\n",
    "\n",
    "    if csv is not None:\n",
    "        st.text(\"\")\n",
    "        st.text(\"\")\n",
    "        st.text(\"\")\n",
    "        st.subheader(\"First few entries of the file : \")\n",
    "        csv_file = pd.read_csv(csv)\n",
    "        st.write(csv_file.head())\n",
    "        pred = predict_time_to_erupt(csv_file)\n",
    "\n",
    "        st.text(\"\")\n",
    "        st.text(\"\")\n",
    "\n",
    "\n",
    "        col_1, col_2 = st.beta_columns(2)\n",
    "        col_2.subheader('Visualize')\n",
    "        vizualize = col_2.radio('', ['No', 'Yes'])\n",
    "\n",
    "        col_1.subheader(\"Time to eruption\")\n",
    "        col_1.text(\"{} centi-seconds\".format(str(pred)))\n",
    "        hm = ms_hm(pred)\n",
    "        col_1.text('Which is approximately {} hours {} minutes'.format(hm['hours'], hm['minutes']))\n",
    "\n",
    "        if vizualize=='Yes':\n",
    "            with st.spinner(text='Please wait while we plot'):\n",
    "                csv_file.plot(subplots=True, layout=(5,2), figsize=(20,10), title=\"Sensor data for the given segment\")\n",
    "                st.text(\"\")\n",
    "                st.text(\"\")\n",
    "                st.pyplot(plt)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. helper.py\n",
    "Contains a small helper function to convert centi seconds to hours-minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_hm(value):\n",
    "    \"\"\"\n",
    "    This function will convert from 10-milliseconds(centi second) format to\n",
    "    hours and minutes.\n",
    "    \"\"\"\n",
    "    no_of_ten_msecs = value\n",
    "    no_of_msecs = 10 * value\n",
    "    no_of_secs = int(no_of_msecs/1000)\n",
    "    no_of_hours = int(no_of_secs/3600)\n",
    "    no_of_mins = int((no_of_secs % 3600)/60)\n",
    "    hm ={'hours' : no_of_hours,\n",
    "         'minutes' : no_of_mins}\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. model.py\n",
    "This file contains the model definition (the final model that will be used for deployement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EnsembleRegressor():  \n",
    "    \"\"\"\n",
    "    This module implements a custom ensemble model.\n",
    "    The training procedure on train set is as follows:\n",
    "        * splits the train set into D1 and D2.(50-50)\n",
    "        * now from this D1 sampling is done with replacement \n",
    "          to create d1,d2,d3....dk(k samples)\n",
    "        * k DecisionTree models are now trained on each of these k samples\n",
    "        (k can be considered as a hyperparameter)\n",
    "        * now the set aside D2 is passed to the k trained models to obtain a k-dimensional feature set\n",
    "        * with the help of these feature set along with D2 targets, a metalearner is trained\n",
    "          which is also a decision tree. This metalearner is our actual model and rest of the base just\n",
    "          baselearner can be considered as feature extractors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_learners = 10, meta_learner = None, oob_size=0.5, max_sample_ratio=None, meta_rs=False, meta_params=None):\n",
    "        self.n_learners = n_learners\n",
    "        self.oob_size = oob_size\n",
    "        self.max_samples = max_samples_ratio if max_sample_ratio is not None else 0.2\n",
    "        self.tree_list = [DecisionTreeRegressor() for i in range(self.n_learners)]\n",
    "        \n",
    "        self.meta_rs = meta_rs\n",
    "        \n",
    "        \n",
    "        \n",
    "        if meta_learner is None or meta_learner == 'decision_tree':\n",
    "            self.meta_learner = DecisionTreeRegressor()\n",
    "        elif meta_learner == 'random_forest':\n",
    "            self.meta_learner = RandomForestRegressor()\n",
    "        elif meta_learner == 'xgboost':\n",
    "            self.meta_learner = XGBRegressor()\n",
    "        elif meta_learner == 'svr':\n",
    "            self.meta_learner = SVR()\n",
    "        elif meta_learner == 'kernel_ridge':\n",
    "            self.meta_learner = KernelRidge()\n",
    "        elif meta_learner == 'bayesian_ridge':\n",
    "            self.meta_learner = BayesianRidge()\n",
    "            \n",
    "        if self.meta_rs:\n",
    "            if not isinstance(meta_params, dict):\n",
    "                raise ValueError(\"Hyperparameter Search Mode requires a dictionary of parameters\")\n",
    "            else:\n",
    "                self.meta_params = meta_params\n",
    "                self.rs_obj = RandomizedSearchCV(self.meta_learner, self.meta_params, cv=5, n_iter=3, n_jobs=3)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _create_sample(self,X,y,fraction):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        indices = random.sample(range(len(X)), int(fraction*len(X)))\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_D1, X_D2, y_D1, y_D2 = train_test_split(X,y,test_size=self.oob_size)\n",
    "        \n",
    "        D2_predlist = []\n",
    "        \n",
    "        for i in range(self.n_learners):\n",
    "            X_temp, y_temp = self._create_sample(X_D1, y_D1, self.max_samples)\n",
    "            self.tree_list[i].fit(X_temp, y_temp)\n",
    "            preds = self.tree_list[i].predict(X_D2)\n",
    "            \n",
    "            D2_predlist.append(preds)\n",
    "        \n",
    "        new_feature_set = np.stack(D2_predlist, axis=1)\n",
    "        \n",
    "        if self.meta_rs:\n",
    "            self.rs_obj.fit(new_feature_set, y_D2)\n",
    "            self.meta_learner = self.rs_obj.best_estimator_\n",
    "        \n",
    "        self.meta_learner.fit(new_feature_set, y_D2)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        D2_predlist = []\n",
    "        \n",
    "        for i in range(self.n_learners):\n",
    "            preds = self.tree_list[i].predict(X)    \n",
    "            D2_predlist.append(preds)\n",
    "        \n",
    "        new_feature_set = np.stack(D2_predlist, axis=1)\n",
    "        return self.meta_learner.predict(new_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. predict.py\n",
    "Contain the two final funtions: prediction & scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from model import *\n",
    "\n",
    "def predict_time_to_erupt(seg_df):\n",
    "    \n",
    "    seg_df = seg_df.fillna(0)\n",
    "    \n",
    "    each_row = []\n",
    "        \n",
    "    for each_column in seg_df.columns:\n",
    "        each_row.append(seg_df[each_column].std())\n",
    "        each_row.append(seg_df[each_column].min())\n",
    "        each_row.append(seg_df[each_column].max())\n",
    "        each_row.append(seg_df[each_column].quantile(.3))\n",
    "        each_row.append(seg_df[each_column].quantile(.6))\n",
    "        each_row.append(seg_df[each_column].quantile(.8))\n",
    "        each_row.append(seg_df[each_column].quantile(.9))\n",
    "        each_row.append(seg_df[each_column].kurt())\n",
    "    \n",
    "    features = np.array(each_row).reshape(1,-1)\n",
    "    \n",
    "    with open('custEnsemblexgb.pkl', 'rb') as f:\n",
    "        best_estimator = pickle.load(f)\n",
    "        \n",
    "    preds = best_estimator.predict(features)\n",
    "    \n",
    "    return preds[0]\n",
    "\n",
    "\n",
    "\n",
    "def return_mae(seg_df, y):\n",
    "    \n",
    "    seg_df = seg_df.fillna(0)\n",
    "    \n",
    "    each_row = []\n",
    "        \n",
    "    for each_column in seg_df.columns:\n",
    "        each_row.append(seg_df[each_column].std())\n",
    "        each_row.append(seg_df[each_column].min())\n",
    "        each_row.append(seg_df[each_column].max())\n",
    "        each_row.append(seg_df[each_column].quantile(.3))\n",
    "        each_row.append(seg_df[each_column].quantile(.6))\n",
    "        each_row.append(seg_df[each_column].quantile(.8))\n",
    "        each_row.append(seg_df[each_column].quantile(.9))\n",
    "        each_row.append(seg_df[each_column].kurt())\n",
    "    \n",
    "    features = np.array(each_row).reshape(1,-1)\n",
    "    \n",
    "    with open('custEnsemblexgb.pkl', 'rb') as f:\n",
    "        best_estimator = pickle.load(f)\n",
    "        \n",
    "    preds = best_estimator.predict(features)\n",
    "    \n",
    "    \n",
    "    return mae(preds[0], y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) setup.sh\n",
    "In this file we will specify command to create a new directory streamlit and various initialization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ~/.streamlit/\n",
    "\n",
    "echo \"\\\n",
    "[server]\\n\\\n",
    "port = $PORT\\n\\\n",
    "enableCORS = false\\n\\\n",
    "headless = true\\n\\\n",
    "\\n\\\n",
    "\" > ~/.streamlit/config.toml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Procfile\n",
    "Required to initiate web app and run setup script & streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web: sh setup.sh && streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) runtime.txt \n",
    "For specifying the python version, for environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python-3.7.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) requirements.txt\n",
    "For specifying the version requirement for other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit==0.80.0\n",
    "pandas==1.1.0\n",
    "numpy==1.18.5\n",
    "scikit-learn==0.22.2.post1\n",
    "matplotlib==3.3.0\n",
    "xgboost==1.1.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
