{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_model definition\n",
    "import numpy as np\n",
    "\n",
    "class EnsembleRegressor():  \n",
    "    \"\"\"\n",
    "    This module implements a custom ensemble model.\n",
    "    The training procedure on train set is as follows:\n",
    "        * splits the train set into D1 and D2.(50-50)\n",
    "        * now from this D1 sampling is done with replacement \n",
    "          to create d1,d2,d3....dk(k samples)\n",
    "        * k DecisionTree models are now trained on each of these k samples\n",
    "        (k can be considered as a hyperparameter)\n",
    "        * now the set aside D2 is passed to the k trained models to obtain a k-dimensional feature set\n",
    "        * with the help of these feature set along with D2 targets, a metalearner is trained\n",
    "          which is also a decision tree. This metalearner is our actual model and rest of the base just\n",
    "          baselearner can be considered as feature extractors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_learners = 10, meta_learner = None, oob_size=0.5, max_sample_ratio=None, meta_rs=False, meta_params=None):\n",
    "        self.n_learners = n_learners\n",
    "        self.oob_size = oob_size\n",
    "        self.max_samples = max_samples_ratio if max_sample_ratio is not None else 0.2\n",
    "        self.tree_list = [DecisionTreeRegressor() for i in range(self.n_learners)]\n",
    "        \n",
    "        self.meta_rs = meta_rs\n",
    "        \n",
    "        \n",
    "        \n",
    "        if meta_learner is None or meta_learner == 'decision_tree':\n",
    "            self.meta_learner = DecisionTreeRegressor()\n",
    "        elif meta_learner == 'random_forest':\n",
    "            self.meta_learner = RandomForestRegressor()\n",
    "        elif meta_learner == 'xgboost':\n",
    "            self.meta_learner = XGBRegressor()\n",
    "        elif meta_learner == 'svr':\n",
    "            self.meta_learner = SVR()\n",
    "        elif meta_learner == 'kernel_ridge':\n",
    "            self.meta_learner = KernelRidge()\n",
    "        elif meta_learner == 'bayesian_ridge':\n",
    "            self.meta_learner = BayesianRidge()\n",
    "            \n",
    "        if self.meta_rs:\n",
    "            if not isinstance(meta_params, dict):\n",
    "                raise ValueError(\"Hyperparameter Search Mode requires a dictionary of parameters\")\n",
    "            else:\n",
    "                self.meta_params = meta_params\n",
    "                self.rs_obj = RandomizedSearchCV(self.meta_learner, self.meta_params, cv=5, n_iter=3, n_jobs=3)\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def _create_sample(self,X,y,fraction):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        indices = random.sample(range(len(X)), int(fraction*len(X)))\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_D1, X_D2, y_D1, y_D2 = train_test_split(X,y,test_size=self.oob_size)\n",
    "        \n",
    "        D2_predlist = []\n",
    "        \n",
    "        for i in range(self.n_learners):\n",
    "            X_temp, y_temp = self._create_sample(X_D1, y_D1, self.max_samples)\n",
    "            self.tree_list[i].fit(X_temp, y_temp)\n",
    "            preds = self.tree_list[i].predict(X_D2)\n",
    "            \n",
    "            D2_predlist.append(preds)\n",
    "        \n",
    "        new_feature_set = np.stack(D2_predlist, axis=1)\n",
    "        \n",
    "        if self.meta_rs:\n",
    "            self.rs_obj.fit(new_feature_set, y_D2)\n",
    "            self.meta_learner = self.rs_obj.best_estimator_\n",
    "        \n",
    "        self.meta_learner.fit(new_feature_set, y_D2)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        D2_predlist = []\n",
    "        \n",
    "        for i in range(self.n_learners):\n",
    "            preds = self.tree_list[i].predict(X)    \n",
    "            D2_predlist.append(preds)\n",
    "        \n",
    "        new_feature_set = np.stack(D2_predlist, axis=1)\n",
    "        return self.meta_learner.predict(new_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time_to_erupt(file):\n",
    "    \n",
    "    seg_df = pd.read_csv(file)\n",
    "    seg_df = seg_df.fillna(0)\n",
    "    \n",
    "    each_row = []\n",
    "        \n",
    "    for each_column in seg_df.columns:\n",
    "        each_row.append(seg_df[each_column].std())\n",
    "        each_row.append(seg_df[each_column].min())\n",
    "        each_row.append(seg_df[each_column].max())\n",
    "        each_row.append(seg_df[each_column].quantile(.3))\n",
    "        each_row.append(seg_df[each_column].quantile(.6))\n",
    "        each_row.append(seg_df[each_column].quantile(.8))\n",
    "        each_row.append(seg_df[each_column].quantile(.9))\n",
    "        each_row.append(seg_df[each_column].kurt())\n",
    "    \n",
    "    features = np.array(each_row).reshape(1,-1)\n",
    "    \n",
    "    with open('custEnsemblexgb.pkl', 'rb') as f:\n",
    "        best_estimator = pickle.load(f)\n",
    "        \n",
    "    preds = best_estimator.predict(features)\n",
    "    \n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mae(file, y):\n",
    "    \n",
    "    seg_df = pd.read_csv(file)\n",
    "    seg_df = seg_df.fillna(0)\n",
    "    \n",
    "    each_row = []\n",
    "        \n",
    "    for each_column in seg_df.columns:\n",
    "        each_row.append(seg_df[each_column].std())\n",
    "        each_row.append(seg_df[each_column].min())\n",
    "        each_row.append(seg_df[each_column].max())\n",
    "        each_row.append(seg_df[each_column].quantile(.3))\n",
    "        each_row.append(seg_df[each_column].quantile(.6))\n",
    "        each_row.append(seg_df[each_column].quantile(.8))\n",
    "        each_row.append(seg_df[each_column].quantile(.9))\n",
    "        each_row.append(seg_df[each_column].kurt())\n",
    "    \n",
    "    features = np.array(each_row).reshape(1,-1)\n",
    "    \n",
    "    with open('custEnsemblexgb.pkl', 'rb') as f:\n",
    "        best_estimator = pickle.load(f)\n",
    "        \n",
    "    preds = best_estimator.predict(features)\n",
    "    \n",
    "    \n",
    "    return mae(preds[0], y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
